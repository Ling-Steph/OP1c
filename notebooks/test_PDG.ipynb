{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "21ba84b0",
      "metadata": {
        "id": "21ba84b0"
      },
      "source": [
        "Script to test PDGrapher on chemical/genetic perturbation data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conda env create -f conda-env.yml\n",
        "conda activate pdgrapher\n",
        "pip install pip==23.2.1\n",
        "pip install -r requirements.txt\n",
        "\n",
        "pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html\n",
        "pip install torch-scatter==2.0.9 -f https://data.pyg.org/whl/torch-1.10.1+cu111.html\n",
        "pip install torch-sparse==0.6.12 -f https://data.pyg.org/whl/torch-1.10.1+cu111.html\n",
        "pip install torch-cluster==1.5.9 -f https://data.pyg.org/whl/torch-1.10.1+cu111.html\n",
        "pip install torch-spline-conv==1.2.1 -f https://data.pyg.org/whl/torch-1.10.1+cu111.html\n",
        "pip install torch-geometric==2.0.4 -f https://data.pyg.org/whl/torch-1.10.1+cu111.html\n",
        "pip install torchmetrics==0.9.3\n",
        "pip install lightning==1.9.5\n"
      ],
      "metadata": {
        "id": "6OwQZg9nZQYc",
        "outputId": "a030439e-180f-4143-9f1b-4fa39442bb94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "id": "6OwQZg9nZQYc",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1575329544.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1575329544.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    conda env create -f conda-env.yml\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d7c8afa1",
      "metadata": {
        "id": "d7c8afa1",
        "outputId": "f25e6b49-ba6a-4cc7-ad05-dddb3c386b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pdgrapher'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2971092856.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_num_threads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdgrapher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDGrapher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdgrapher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdgrapher'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path as osp\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "import os\n",
        "import os.path as osp\n",
        "torch.set_num_threads(20)\n",
        "from datetime import datetime\n",
        "from pdgrapher import PDGrapher\n",
        "from pdgrapher import Trainer, Dataset\n",
        "import sys\n",
        "from glob import glob\n",
        "from pdgrapher._utils import get_thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba11b69e",
      "metadata": {
        "id": "ba11b69e"
      },
      "source": [
        "Test on chemical or genetic datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b7fbf8",
      "metadata": {
        "id": "d5b7fbf8"
      },
      "outputs": [],
      "source": [
        "data_type = \"Chemical\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c4dca7d",
      "metadata": {
        "id": "3c4dca7d"
      },
      "source": [
        "Define the corresponding cell lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a58524",
      "metadata": {
        "id": "43a58524"
      },
      "outputs": [],
      "source": [
        "if data_type == \"chemical\":\n",
        "    cell_lines = ['A549', 'MCF7', 'PC3', 'VCAP', 'MDAMB231', 'BT20', 'HT29', 'A375', 'HELA']\n",
        "elif data_type == \"genetic\":\n",
        "    cell_lines = ['PC3', 'YAPC', 'AGS', 'A375', 'HT29', 'A549', 'BICR6', 'U251MG', 'ES2', 'MCF7']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776f7d20",
      "metadata": {
        "id": "776f7d20"
      },
      "source": [
        "Setup parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acaff3ea",
      "metadata": {
        "id": "acaff3ea"
      },
      "outputs": [],
      "source": [
        "use_backward_data = True\n",
        "use_supervision = True #whether to use supervision loss\n",
        "use_intervention_data = True #whether to use cycle loss\n",
        "current_date = datetime.now()\n",
        "n_layers_nn = 1\n",
        "global use_forward_data\n",
        "use_forward_data = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "882b4b4d",
      "metadata": {
        "id": "882b4b4d"
      },
      "source": [
        "Load model and do test and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb367236",
      "metadata": {
        "id": "bb367236"
      },
      "outputs": [],
      "source": [
        "for cell_line in cell_lines:\n",
        "    print(f\"Processing cell line: {cell_line}\")\n",
        "\n",
        "    if data_type == 'chemical':\n",
        "        if cell_line in ['HA1E', 'HT29', 'A375', 'HELA']:\n",
        "            use_forward_data = False\n",
        "\n",
        "        #Dataset\n",
        "        dataset = Dataset(\n",
        "            forward_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/data_forward_{cell_line}.pt\",\n",
        "            backward_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/data_backward_{cell_line}.pt\",\n",
        "            splits_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/PDgrapher/data/split_guada/{data_type}/{data_type}/{cell_line}/random/5fold/splits.pt\"\n",
        "        )\n",
        "\n",
        "        edge_index = torch.load(f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/edge_index_{cell_line}.pt\")\n",
        "\n",
        "        #Modify based on folder name\n",
        "        paths = glob('/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/PDgrapher/output/{}/{}_corrected_pos_emb/*'.format(data_type, cell_line))\n",
        "\n",
        "    elif data_type == 'genetic':\n",
        "        if cell_line in ['ES2', 'BICR6', 'YAPC', 'AGS', 'U251MG', 'HT29', 'A375']:\n",
        "            use_forward_data = False\n",
        "\n",
        "        #Dataset\n",
        "        dataset = Dataset(\n",
        "            forward_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/data_forward_{cell_line}.pt\",\n",
        "            backward_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/data_backward_{cell_line}.pt\",\n",
        "            splits_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/PDgrapher/data/split_guada/{data_type}/{data_type}/{cell_line}/random/5fold/splits.pt\"\n",
        "        )\n",
        "\n",
        "        edge_index = torch.load(f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/edge_index_{cell_line}.pt\")\n",
        "\n",
        "        #Modify based on folder name\n",
        "        paths = glob('/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/PDgrapher/output/{}/{}_corrected_pos_emb/*'.format(data_type, cell_line))\n",
        "\n",
        "    for path in paths:\n",
        "\n",
        "        outdir = './results_pdgrapher/{}/val/'.format(data_type)\n",
        "        path_model = path\n",
        "\n",
        "        n_layers_gnn = int(path.split('/')[-1].split('_')[2])\n",
        "\n",
        "        all_recall_at_1 = {'test':[], 'val':[]}\n",
        "        all_recall_at_10 = {'test':[], 'val':[]}\n",
        "        all_recall_at_100 = {'test':[], 'val':[]}\n",
        "        all_recall_at_1000 = {'test':[], 'val':[]}\n",
        "        all_perc_partially_accurate_predictions = {'test':[], 'val':[]}\n",
        "        all_rankings = {'test':[], 'val':[]}\n",
        "\n",
        "        for fold in range(1,6):\n",
        "            #Instantiates model\n",
        "            model = PDGrapher(edge_index, model_kwargs={\"n_layers_nn\": 1, \"n_layers_gnn\": n_layers_gnn, \"num_vars\": dataset.get_num_vars(),\n",
        "                                                        },\n",
        "                                        response_kwargs={'train': True},\n",
        "                                        perturbation_kwargs={'train':True})\n",
        "\n",
        "            # restore response prediction\n",
        "            save_path = osp.join(path, '_fold_{}_response_prediction.pt'.format(fold))\n",
        "            checkpoint = torch.load(save_path)\n",
        "            model.response_prediction.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "            # restore Perturbation discovery\n",
        "            save_path = osp.join(path, '_fold_{}_perturbation_discovery.pt'.format(fold))\n",
        "            checkpoint = torch.load(save_path)\n",
        "            model.perturbation_discovery.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "            #loads fold-specific dataset\n",
        "            device = torch.device('cuda')\n",
        "\n",
        "            dataset.prepare_fold(fold)\n",
        "\n",
        "            thresholds = get_thresholds(dataset)\n",
        "            thresholds = {k: v.to(device) if v is not None else v for k, v in thresholds.items()}\n",
        "\n",
        "            model.response_prediction.edge_index = model.response_prediction.edge_index.to(device)\n",
        "            model.perturbation_discovery.edge_index = model.perturbation_discovery.edge_index.to(device)\n",
        "            model.perturbation_discovery = model.perturbation_discovery.to(device)\n",
        "\n",
        "            model.perturbation_discovery.eval()\n",
        "            model.response_prediction.eval()\n",
        "\n",
        "            (\n",
        "                        train_loader_forward, train_loader_backward,\n",
        "                        val_loader_forward, val_loader_backward,\n",
        "                        test_loader_forward, test_loader_backward\n",
        "                    ) = dataset.get_dataloaders(num_workers = 20, batch_size = 1)\n",
        "\n",
        "            recall_at_1 = []\n",
        "            recall_at_10 = []\n",
        "            recall_at_100 = []\n",
        "            recall_at_1000 = []\n",
        "            perc_partially_accurate_predictions = []\n",
        "            rankings = []\n",
        "            n_non_zeros = 0\n",
        "\n",
        "            ####ON TEST SET\n",
        "            for data in test_loader_backward:\n",
        "                pred_backward_m2 = model.perturbation_discovery(torch.concat([data.diseased.view(-1, 1).to(device), data.treated.view(-1, 1).to(device)], 1), data.batch.to(device), mutilate_mutations=data.mutations.to(device), threshold_input=thresholds)\n",
        "                out = pred_backward_m2\n",
        "\n",
        "                num_nodes = int(data.num_nodes / len(torch.unique(data.batch)))\n",
        "\n",
        "                correct_interventions = set(torch.where(data.intervention.detach().cpu().view(-1, num_nodes))[1].tolist())\n",
        "                predicted_interventions = torch.argsort(out.detach().cpu().view(-1, num_nodes), descending=True)[0, :].tolist()\n",
        "\n",
        "                for ci in list(correct_interventions):\n",
        "                    rankings.append(1 - (predicted_interventions.index(ci) / num_nodes))\n",
        "\n",
        "                recall_at_1.append(len(set(predicted_interventions[:1]).intersection(correct_interventions)) / len(correct_interventions))\n",
        "                recall_at_10.append(len(set(predicted_interventions[:10]).intersection(correct_interventions)) / len(correct_interventions))\n",
        "                recall_at_100.append(len(set(predicted_interventions[:100]).intersection(correct_interventions)) / len(correct_interventions))\n",
        "                recall_at_1000.append(len(set(predicted_interventions[:1000]).intersection(correct_interventions)) / len(correct_interventions))\n",
        "\n",
        "                jaccards = len(correct_interventions.intersection(predicted_interventions[:len(correct_interventions)])) / len(correct_interventions.union(predicted_interventions))\n",
        "\n",
        "                if jaccards != 0:\n",
        "                    n_non_zeros += 1\n",
        "\n",
        "            all_recall_at_1['test'].append(np.mean(recall_at_1))\n",
        "            all_recall_at_10['test'].append(np.mean(recall_at_10))\n",
        "            all_recall_at_100['test'].append(np.mean(recall_at_100))\n",
        "            all_recall_at_1000['test'].append(np.mean(recall_at_1000))\n",
        "            all_rankings['test'].append(np.mean(rankings))\n",
        "            all_perc_partially_accurate_predictions['test'].append(100 * n_non_zeros/len(test_loader_backward))\n",
        "            print('fold {}/5'.format(fold))\n",
        "\n",
        "            ####ON VALIDATION SET\n",
        "            recall_at_1 = []\n",
        "            recall_at_10 = []\n",
        "            recall_at_100 = []\n",
        "            recall_at_1000 = []\n",
        "            perc_partially_accurate_predictions = []\n",
        "            rankings = []\n",
        "            n_non_zeros = 0\n",
        "\n",
        "\n",
        "            for data in val_loader_backward:\n",
        "                pred_backward_m2 = model.perturbation_discovery(torch.concat([data.diseased.view(-1, 1).to(device), data.treated.view(-1, 1).to(device)], 1), data.batch.to(device), mutilate_mutations=data.mutations.to(device), threshold_input=thresholds)\n",
        "                out = pred_backward_m2\n",
        "\n",
        "                num_nodes = int(data.num_nodes / len(torch.unique(data.batch)))\n",
        "\n",
        "\n",
        "                correct_interventions = set(torch.where(data.intervention.detach().cpu().view(-1, num_nodes))[1].tolist())\n",
        "                predicted_interventions = torch.argsort(out.detach().cpu().view(-1, num_nodes), descending=True)[0, :].tolist()\n",
        "\n",
        "                for ci in list(correct_interventions):\n",
        "                    rankings.append(1 - (predicted_interventions.index(ci) / num_nodes))\n",
        "\n",
        "                recall_at_1.append(len(set(predicted_interventions[:1]).intersection(correct_interventions)) / len(correct_interventions))\n",
        "                recall_at_10.append(len(set(predicted_interventions[:10]).intersection(correct_interventions)) / len(correct_interventions))\n",
        "                recall_at_100.append(len(set(predicted_interventions[:100]).intersection(correct_interventions)) / len(correct_interventions))\n",
        "                recall_at_1000.append(len(set(predicted_interventions[:1000]).intersection(correct_interventions)) / len(correct_interventions))\n",
        "\n",
        "\n",
        "                jaccards = len(correct_interventions.intersection(predicted_interventions[:len(correct_interventions)])) / len(correct_interventions.union(predicted_interventions))\n",
        "\n",
        "                if jaccards != 0:\n",
        "                    n_non_zeros += 1\n",
        "\n",
        "\n",
        "            all_recall_at_1['val'].append(np.mean(recall_at_1))\n",
        "            all_recall_at_10['val'].append(np.mean(recall_at_10))\n",
        "            all_recall_at_100['val'].append(np.mean(recall_at_100))\n",
        "            all_recall_at_1000['val'].append(np.mean(recall_at_1000))\n",
        "            all_rankings['val'].append(np.mean(rankings))\n",
        "            all_perc_partially_accurate_predictions['val'].append(100 * n_non_zeros/len(test_loader_backward))\n",
        "            print('fold {}/5'.format(fold))\n",
        "\n",
        "\n",
        "        log = open(osp.join(outdir, f'{cell_line}_{n_layers_gnn}_final_performance_metrics_within.txt'), 'w')\n",
        "        log.write('\\n\\nVALIDATION SET\\n')\n",
        "        log.write('recall@1: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_1['val']), np.std(all_recall_at_1['val'])))\n",
        "        log.write('recall@10: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_10['val']), np.std(all_recall_at_10['val'])))\n",
        "        log.write('recall@100: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_100['val']), np.std(all_recall_at_100['val'])))\n",
        "        log.write('recall@1000: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_1000['val']), np.std(all_recall_at_1000['val'])))\n",
        "        log.write('percentage of samples with partially accurate predictions: {:.2f}±{:.2f}\\n'.format(np.mean(all_perc_partially_accurate_predictions['val']), np.std(all_perc_partially_accurate_predictions['val'])))\n",
        "        log.write('ranking score: {:.2f}±{:.2f}\\n'.format(np.mean(all_rankings['val']), np.std(all_rankings['val'])))\n",
        "\n",
        "        log.write('--------------------------\\n')\n",
        "        log.write('All metric datapoints:\\n')\n",
        "        log.write('recall@1: {}\\n'.format(all_recall_at_1['val']))\n",
        "        log.write('recall@10: {}\\n'.format(all_recall_at_10['val']))\n",
        "        log.write('recall@100: {}\\n'.format(all_recall_at_100['val']))\n",
        "        log.write('recall@1000: {}\\n'.format(all_recall_at_1000['val']))\n",
        "        log.write('percentage of samples with partially accurate predictions: {}\\n'.format(all_perc_partially_accurate_predictions['val']))\n",
        "        log.write('ranking score: {}\\n'.format(all_rankings['val']))\n",
        "\n",
        "        log.write('\\n\\n----------------------\\n')\n",
        "        log.write('\\n\\nTEST SET\\n')\n",
        "        log.write('recall@1: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_1['test']), np.std(all_recall_at_1['test'])))\n",
        "        log.write('recall@10: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_10['test']), np.std(all_recall_at_10['test'])))\n",
        "        log.write('recall@100: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_100['test']), np.std(all_recall_at_100['test'])))\n",
        "        log.write('recall@1000: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_1000['test']), np.std(all_recall_at_1000['test'])))\n",
        "        log.write('percentage of samples with partially accurate predictions: {:.2f}±{:.2f}\\n'.format(np.mean(all_perc_partially_accurate_predictions['test']), np.std(all_perc_partially_accurate_predictions['test'])))\n",
        "        log.write('ranking score: {:.2f}±{:.2f}\\n'.format(np.mean(all_rankings['test']), np.std(all_rankings['test'])))\n",
        "\n",
        "        log.write('--------------------------\\n')\n",
        "        log.write('All metric datapoints:\\n')\n",
        "        log.write('recall@1: {}\\n'.format(all_recall_at_1['test']))\n",
        "        log.write('recall@10: {}\\n'.format(all_recall_at_10['test']))\n",
        "        log.write('recall@100: {}\\n'.format(all_recall_at_100['test']))\n",
        "        log.write('recall@1000: {}\\n'.format(all_recall_at_1000['test']))\n",
        "        log.write('percentage of samples with partially accurate predictions: {}\\n'.format(all_perc_partially_accurate_predictions['test']))\n",
        "        log.write('ranking score: {}\\n'.format(all_rankings['test']))\n",
        "\n",
        "        log.close()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}